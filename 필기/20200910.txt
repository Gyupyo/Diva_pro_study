[주간보고서]
- 2.4.5번째가 아직 진행이 안된건가요?
- '기존 프로젝트 및 관련 기술 학습' 10월까지니까 적을 것
- '격주' 니까 세분화해서 진행한 내용을 적자
- 진행이 안된 것들은 특별한 이유가 있나요?
이해부족으로 '미진행'이 아니라 '미완료'라는 표현을 사용할 것
============================================
[발표]
- 자연어 처리에 대해서 쭉 조사를 하고 발표를 하고 있는데... 분량이 얼마나 되나요?
너무 길면 발표 끝나고 discussion이 아예 이루어지지 않으니까 관련된 것 위주로 좀 더 추려서 하자
우리가 하고 있는 딥러닝 기반 방법 위주로 다루도록 하자

(word embedding 부분으로 점프!)

리허설만 했나요? 아니면 이해도 했나요?
아직입니다!
-----------------------------------------------
현우형 질문: Word embedding은 어떤식으로 학습이 이루어지나요?
답변 : 학습에 의해서 정해지는 거고요. 딥러닝 방법과는 크게 다르지 않습니다.
p를 어떻게 구하냐에 대한 답변은 머신러닝이 공부가 되어야 와닿는 답변일 것이다

변환하는 방법에는 ASCII code 값 나열하는 방법도 있지만 이 경우, 연관성이 완전히 사라진다. 
순차성만 남는다 그래서 embedding을 사용한다

p를 잘 구해야한다. p에서 p`로 되는 것은 인코딩 그 반대는 디코딩 과정이다
(p`도 결국 찾아야 한다) - p'는 p의 행과 열을 서로 뒤집은 벡터(transposit matrix)
따라서 어떻게 인코딩을 할 것인지도 중요하다
어떻게 정할 것인가? Training set에 따라서 인코딩 되는 것은 달라질 수 있다.
ex) 영화와 관련된 NLP를 하고 싶다. 그래서 Word Embedding를 하고 싶다할때 쓰는 임베딩 방법/결과와
판결문과 관련된 NLP를 할 때 진행되는 Word Embedding의 임베딩 방법/결과는 다른 방법을 사용할 것이다.

따라서, 다른 값을 같게 되겠지만, 비슷한 위치에 있게 된다는 원리이다.
그것이 색깔별로 다른 화면이다.

추가 설명) 같은 문장에 같이 나오는 단어라면 연관성이 있다고 본다. 하지만 내가 가지고 있는 
training set의 문장들이 어떻게 되어있느냐에 따라서 연관성에 대한 척도가 달라질 수 있고,
벡터가 임베딩 된 결과가 조금조금 씩 달라질 수 있다.
-----------------------------------------------
언어모델 
Markov 확률 기반의 언어 모델 >>> "이것을 사용할 것인가?" (RNN으로 점프)
-----------------------------------------------
RNN 기반의 언어 모델
교수님 질문 : 
오른쪽 그림이 RNN!
오른쪽 그림에서 hidden node(가상의 노드?)는 하나이다 'Previous step hidden layer'
이유:
" output vector는 값이다. 노드는 실제 물리적으로 존재해야하는 거고, 값은 값일 뿐이다
노드를 object라고 본다면, 그 안에 있는 멤버함수가 반환하는 값이 output vector이다
실제로 존재하는 것은 왼쪽 그림이다. 박스가 노드를 나타낸다. 
문자는 노드를 가리키는 것이 아니고, 노드에서 계산된 값을 의미한다.
따라서 왼쪽 그림이 RNN 그 자체이다. 오른쪽 그림은 이해를 돕기위한 그림이다. 

실제 노드는 output, hidden, input layer에 각 하나씩 존재한다. 나머지는 가상의 노드이다.
같은 색의 화살표는 같은 것이다(? 제대로 못 들음)
hidden layer안의 박스에 있는 수치들은 activation function의 결과값이자, 엠베딩 된 결과값이다.
ouput layer안의 박스에 있는 수치들도 같은 원리이다. 
단, 이것은 Word Embedding이 아니다!!! 그 역할을 할 수 있지만, 같지 않다
그림의 예시는 단어를 중간까지만 보여주고, 나머지 부분을 유추하는 모델을 만드는 것이다
output layer에 있는 값들에서 대소 관계가 우리가 원하는 관계가 아니다. 
따라서 이 상태는 학습이 된 상태가 아니다.
예를 들어 'e'같은 경우 output layer의 요소 중 첫번째 요소가 가장 커야하지만, 두번째가 크다.
따라서 error가 발생한 것이며, 이는 나중에 학습을 통해 조정해야 한다.
학습에서 하는 것은 W값들을 업데이트 하는 것이다. 그 학습을 통해서 최종적으로 e,l,l,o가 출력되도록 하는 것이다.

따라서 왼쪽 그림이 나오는 것이다
왼쪽 그림에는 나와있지 않지만, bh는 bias(상수)이다. 하지만 이것은 학습과정을 나타낸 것이 아니다.
오른쪽 그림은 왼쪽 그림이 반복되는 과정을 나타내는 그림이다.
따라서 오른쪽 그림은 학습하는 것을 보여주는 그림이 아니라, 단순히 출력+계산과정을 보여주는 그림이다
(classification보다는 regression에 가깝다)

RNN >>> BERT >> LSTM (이해 순서)

아직 RNN에 대해 설명하지 않은 것은?
1. RNN의 학습 알고리즘은?
2. RNN이 사용되는 방법들은 여러가지가 있다. 그것은 시나리오에 따라 다르다.
----------------------------------------------------
우리는 label에대한 classification을 해야하니까 fine-tuning을 사용하는 것이고, 
그래서 BERT를 사용하는 것이다. 

그래서 CNN과 BERT의 동작원리를 이해해야 한다
머신러닝에 대한 학습이 필요하다 CNN부터 BERT까지
예습해와라!!!

교수님의 머신러닝 자료 뒷부분에 CNN이 있다. 거기까지는 스터디를 해와라.
CNN은 질의응답 방식으로 진행하는 방식으로 하고 건너뛴다.

Transfromer와 BERT는 조교와 나중에 상의해서 결정